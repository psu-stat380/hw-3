---
title: "Homework 3"
author: "[Marc Hughes]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
# format: html
format: pdf
---

[Link to the Github repository](https://github.com/psu-stat380/hw-3)

---

::: {.callout-important style="font-size: 0.8em;"}
## Due: Thu, Mar 2, 2023 @ 11:59pm

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::

For this assignment, we will be using the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset from the UCI Machine Learning Repository. The dataset consists of red and white _vinho verde_ wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests

We will be using the following libraries:

```{R}
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(car)
library(glmnet)
library(corrplot)
library(broom)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 50 points
Regression with categorical covariate and $t$-Test
:::

###### 1.1 (5 points)

Read the wine quality datasets from the specified URLs and store them in data frames `df1` and `df2`.

```{R}
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

url2 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"


df1 <- read.csv(url1, sep = ';')
df2 <- read.csv(url2, sep = ';')
```

---

###### 1.2 (5 points)

Perform the following tasks to prepare the data frame `df` for analysis:

1. Combine the two data frames into a single data frame `df`, adding a new column called `type` to indicate whether each row corresponds to white or red wine. 
1. Rename the columns of `df` to replace spaces with underscores
1. Remove the columns `fixed_acidity` and `free_sulfur_dioxide`
1. Convert the `type` column to a factor
1. Remove rows (if any) with missing values.


```{R}
# adding new column to both data frames that will distinguish the two types of wine when bound together
df1$type = "white"
df2$type = "red"

# binding both data frames
df <- rbind(df1, df2) 

# replacing the periods with and underscore
colnames(df) <- gsub('\\.', '_', colnames(df))

# removing columns 'fixed_acidity' and 'free_sulfur_dioxide' from the data frame
df <- df %>%
  select(!fixed_acidity & !free_sulfur_dioxide)

# changing the 'type' column to a factor
df$type <- factor(df$type)

# dropping any missing values
df <- df %>%
  drop_na()

```


Your output to `R dim(df)` should be
```
[1] 6497   11
```



---

###### 1.3 (20 points)

Recall from STAT 200, the method to compute the $t$ statistic for the the difference in means (with the equal variance assumption)

1. Using `df` compute the mean of `quality` for red and white wine separately, and then store the difference in means as a variable called `diff_mean`. 

2. Compute the pooled sample variance and store the value as a variable called `sp_squared`. 

3. Using `sp_squared` and `diff_mean`, compute the $t$ Statistic, and store its value in a variable called `t1`.


```{R}
# creating temporary df to calculate mean
temp_df <-
  df %>%
  group_by(type) %>%
  summarise("quality_mean" = mean(quality))

# calculating mean
diff_mean <- abs(temp_df$quality_mean[temp_df$type == "white"] - temp_df$quality_mean[temp_df$type == "red"])

# finding lengths
n1 <- length(df$quality[df$type == "white"])
n2 <- length(df$quality[df$type == "red"])

var1 <- var(df$quality[df$type == "white"])
var2 <- var(df$quality[df$type == "red"])

# manually calculating sp_squared
sp_squared <- ((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2)

# calculating standard deviation
sd1 <- sd(df$quality[df$type == "white"])
sd2 <- sd(df$quality[df$type == "red"])

# calculating the t-statistic
t1 <-  diff_mean / sqrt(sp_squared*(1/n1 + 1/n2))
```


---

###### 1.4 (10 points)

Equivalently, R has a function called `t.test()` which enables you to perform a two-sample $t$-Test without having to compute the pooled variance and difference in means. 

Perform a two-sample t-test to compare the quality of white and red wines using the `t.test()` function with the setting `var.equal=TRUE`. Store the t-statistic in `t2`.

```{R}
# using 't.test()' function to calculate the t-statistic
t_test <- t.test(df$quality[df$type == "white"], df$quality[df$type == "red"], var.equal = TRUE)
t2 <- t_test$statistic
```

---

###### 1.5 (5 points)

Fit a linear regression model to predict `quality` from `type` using the `lm()` function, and extract the $t$-statistic for the `type` coefficient from the model summary. Store this $t$-statistic in `t3`.

```{R}
fit <- lm(quality ~ type, data = df)
t3 <- summary(fit)$coefficients[2, "t value"]
```


---

###### 1.6  (5 points)

Print a vector containing the values of `t1`, `t2`, and `t3`. What can you conclude from this? Why?

```{R}
c(t1, t2, t3)
```

You can conclude that all three of the methods are valid ways of extracting the t-statistic due to all t values being the exact same regardless of the method. In addition, this shows that the t-statistic is significant which allows us to reject the null hypothesis.


<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 25 points
Collinearity
:::


---

###### 2.1 (5 points)

Fit a linear regression model with all predictors against the response variable `quality`. Use the `broom::tidy()` function to print a summary of the fitted model. What can we conclude from the model summary?


```{R}
full_model <- lm(quality ~ ., data = df)
broom::tidy(summary(full_model))
```

We can conclude that all of the p-values are significant which means we reject the null hypothesis and accept the alternative. In addition, the summary shows, evident by the "statistic" column, that only a select few of the t values are statistically significant.

---

###### 2.2 (10 points)

Fit two **simple** linear regression models using `lm()`: one with only `citric_acid` as the predictor, and another with only `total_sulfur_dioxide` as the predictor. In both models, use `quality` as the response variable. How does your model summary compare to the summary from the previous question?


```{R}
model_citric <- lm(quality ~ citric_acid, data = df)
summary(model_citric)
```

```{R}
model_sulfur <- lm(quality ~ total_sulfur_dioxide, data = df)
summary(model_sulfur)
```

The t value of the "citric_acid" predictor under the model_citric model shows a large increase to the t value shown in the model with all predictors. The increase was so much so that "citric_acid" actually has a significant t value in "model_critic". On the other hand, the t value of the "total_sulfur_dioxide" predictor is even smaller and remains insignificant.

---

###### 2.3 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R}
# creating data frame with only numeric columns
df_numeric <- df %>% 
  keep(is.numeric)

# creating correlation matrix
cor_mat <- cor(df_numeric)
# visualizing the correlation matrix
corrplot(cor_mat, type = 'upper')

```



---

###### 2.4 (5 points)

Compute the variance inflation factor (VIF) for each predictor in the full model using `vif()` function. What can we conclude from this?


```{R}
vif(full_model)
```

We can conclude that predictors like; 'volatile_acidity', 'residual_sugar', 'total_sulfur_dioxide', 'density', 'alcohol', and 'type' all of relatively high variance inflation factors. Having a high variance inflation means that the predictors listed are highly correlated with other variables in the model. This means that values like the t-statistic and p-value vastly different for variables with a high VIF depending on the inclusion of other highly correlated variables within the model.


<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 40 points

Variable selection
:::


---

###### 3.1 (5 points)

Run a backward stepwise regression using a `full_model` object as the starting model. Store the final formula in an object called `backward_formula` using the built-in `formula()` function in R

```{R}
full_model <- lm(quality ~ ., data = df)

backward_formula <- step(full_model, direction = "backward", scope=formula(full_model))
```

---

###### 3.2 (5 points)

Run a forward stepwise regression using a `null_model` object as the starting model. Store the final formula in an object called `forward_formula` using the built-in `formula()` function in R

```{R}
null_model <- lm(quality ~ 1, data = df)

forward_formula <- step(null_model, direction = "forward", scope = formula(full_model))
```



---

###### 3.3  (10 points)

1. Create a `y` vector that contains the response variable (`quality`) from the `df` dataframe. 

2. Create a design matrix `X` for the `full_model` object using the `make_model_matrix()` function provided in the Appendix. 

3. Then, use the `cv.glmnet()` function to perform LASSO and Ridge regression with `X` and `y`.

```{R}
# creating y vector
y <- df$quality
# creating a design matrix
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
# performing lasso regression
lassoReg <- cv.glmnet(x = make_model_matrix(forward_model), y = y, alpha = 1)
lassoReg
# performing ridge regression
ridgeReg <- cv.glmnet(x = make_model_matrix(forward_model), y = y, alpha = 0)
ridgeReg

```

Create side-by-side plots of the ridge and LASSO regression results. Interpret your main findings. 

```{R}
par(mfrow=c(1, 2))

plot(lassoReg)
title("Lasso Plot", line = .3)

plot(ridgeReg)
title("Ridge Plot", line = .3)
```

The LASSO plot suggests the lambda that is most optimal for the minimization of mean-squared error. According to the plot, this model has 4 variables. The ridge plot, on the other hand, uses a lambda of approximately -2 to minimize mean-squared error and includes all variables because ridge regression does not do variable selection.

---

###### 3.4  (5 points)

Print the coefficient values for LASSO regression at the `lambda.1se` value? What are the variables selected by LASSO? 

Store the variable names with non-zero coefficients in `lasso_vars`, and create a formula object called `lasso_formula` using the `make_formula()` function provided in the Appendix. 

```{R}
lasso_coef <- coef(lassoReg, s = "lambda.1se")
lasso_coef

lasso_vars <- rownames(lasso_coef)[which(abs(lasso_coef) > 0)][-1] # exclude the intercept term
lasso_vars

make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

lasso_formula <- make_formula(lasso_vars)

```

The variables selected are "volatile_acidity", "residual_sugar", "sulphates", and "alcohol". 

---

###### 3.5  (5 points)

Print the coefficient values for ridge regression at the `lambda.1se` value? What are the variables selected here? 

Store the variable names with non-zero coefficients in `ridge_vars`, and create a formula object called `ridge_formula` using the `make_formula()` function provided in the Appendix. 

```{R}
ridge_coef <- coef(ridgeReg, s = "lambda.1se")
ridge_coef

ridge_vars <- rownames(ridge_coef)[which(abs(ridge_coef) > 0)][-1] # exclude the intercept term
ridge_vars

make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

ridge_formula <- make_formula(ridge_vars)

```

All the variables are selected; "volatile_acidity", "citric_acid", "residual_sugar", "chlorides", "total_sulfur_dioxide", "density", "pH", "sulphates", "alcohol", and "type".

---

###### 3.6  (10 points)

What is the difference between stepwise selection, LASSO and ridge based on you analyses above?


Stepwise selection uses 'AIC' to do variable selection and seems to retain more variables than LASSO regression. On the other hand, LASSO regression uses mean-squared error to do variable selection. Finally, ridge regression does not do variable selection whatsoever.





<br><br><br><br>
<br><br><br><br>
---

## Question 4
::: {.callout-tip}
## 70 points

Variable selection
:::

---

###### 4.1  (5 points)

Excluding `quality` from `df` we have $10$ possible predictors as the covariates. How many different models can we create using any subset of these $10$ coavriates as possible predictors? Justify your answer. 

We can create 2^10 different models, so 1024 different models. This is because order doesn't matter and you can either include a predictor or exclude a predictor.

---


###### 4.2  (20 points)

Store the names of the predictor variables (all columns except `quality`) in an object called `x_vars`.

```{R}
# storing predictor variables
x_vars <- colnames(df %>% select(-quality))
```

Use: 

* the `combn()` function (built-in R function) and 
* the `make_formula()` (provided in the Appendix) 

to **generate all possible linear regression formulas** using the variables in `x_vars`. This is most optimally achieved using the `map()` function from the `purrr` package.

```{R}
formulas <- map(
  1:length(x_vars),
  function(x){
    vars <- combn(x_vars, x, simplify = FALSE)
    map(vars, make_formula) 
  }
) %>% unlist()
```

If your code is right the following command should return something along the lines of:

```{R}
sample(formulas, 4) %>% as.character()
# Output:
# [1] "quality ~ volatile_acidity + residual_sugar + density + pH + alcohol"                                                 
# [2] "quality ~ citric_acid"                                                                                                
# [3] "quality ~ volatile_acidity + citric_acid + residual_sugar + total_sulfur_dioxide + density + pH + sulphates + alcohol"
# [4] "quality ~ citric_acid + chlorides + total_sulfur_dioxide + pH + alcohol + type"  
```

---

###### 4.3  (10 points)
Use `map()` and `lm()` to fit a linear regression model to each formula in `formulas`, using `df` as the data source. Use `broom::glance()` to extract the model summary statistics, and bind them together into a single tibble of summaries using the `bind_rows()` function from `dplyr`.

```{R}
models <- map(formulas, ~ lm(formula = .x, data = df))
summaries <- bind_rows(map(models, broom::glance)) 

summaries
```



---


###### 4.4  (5 points)

Extract the `adj.r.squared` values from `summaries` and use them to identify the formula with the _**highest**_ adjusted R-squared value.

```{R}
extraction1 <- summaries$adj.r.squared

```

Store resulting formula as a variable called `rsq_formula`.

```{R}
rsq_formula <- formulas[which.max(extraction1)]
```

---

###### 4.5  (5 points)

Extract the `AIC` values from `summaries` and use them to identify the formula with the **_lowest_** AIC value.


```{R}
extraction2 <- summaries$AIC
```

Store resulting formula as a variable called `aic_formula`.


```{R}
aic_formula <- formulas[which.min(extraction2)]
```

---

###### 4.6  (15 points)

Combine all formulas shortlisted into a single vector called `final_formulas`.

```{R}
null_formula <- formula(null_model)
full_formula <- formula(full_model)

final_formulas <- c(
  null_formula,
  full_formula,
  backward_formula,
  forward_formula,
  lasso_formula, 
  ridge_formula,
  rsq_formula,
  aic_formula
)
```

* Are `aic_formula` and `rsq_formula` the same? How do they differ from the formulas shortlisted in question 3?

The 'aic_formula' and 'rsq_formula' are not the same and they differ from the some of the other formulas in the way in which they were found and what they represent. For example, 'rsq_formula' was found by simply extracting the formula with the highest adjusted r-squared from the model summaries and matches no other formulas shortlisted. 'aic_formula', on the other hand, actually matches both the 'backward_formula' and the 'forward_formula'. This is because the 'aic_formula' was found by finding the formula with the lowest AIC which is exactly what backward and forward selection achieves.

* Which of these is more reliable? Why? 

The 'aic_formula' is more reliable than 'rsq_formula' because it uses an already tried and tested way of finding an optimal model which is choosing a model that minimizes AIC.

* If we had a dataset with $10,000$ columns, which of these methods would you consider for your analyses? Why?

If we had a dataset with $10,000$ columns I would choose to use LASSO regression because they can handle high-dimensional datasets LASSO engages in feature selection so it can reduce the amount of parameters that would otherwise make the regression model much more computationally intensive.

---

###### 4.7  (10 points)


Use `map()` and `glance()` to extract the `sigma, adj.r.squared, AIC, df`, and `p.value` statistics for each model obtained from `final_formulas`. Bind them together into a single data frame `summary_table`. Summarize your main findings.

```{R}
summary_table <- map(
  final_formulas, 
  function(x) {
    model <- lm(x, data = df)
    glance(model) %>%
      select(sigma, adj.r.squared, AIC, df, p.value) %>%
      mutate(formula = as.character(x))
  }
) %>% bind_rows()

summary_table %>% knitr::kable()
```





:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---


# Appendix


#### Convenience function for creating a formula object

The following function which takes as input a vector of column names `x` and outputs a `formula` object with `quality` as the response variable and the columns of `x` as the covariates. 

```R
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

#### Convenience function for `glmnet`

The `make_model_matrix` function below takes a `formula` as input and outputs a **rescaled** model matrix `X` in a format amenable for `glmnet()`

```R
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```




::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::